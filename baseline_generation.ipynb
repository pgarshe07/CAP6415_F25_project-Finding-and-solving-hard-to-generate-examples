{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline Generation Notebook\n",
        "\n",
        "## Project: Finding and Solving Hard-to-Generate Examples - Speed Bumps\n",
        "\n",
        "This notebook implements the baseline image generation pipeline using Stable Diffusion XL (SDXL)\n",
        "to generate images of speed bumps and related roadway fixtures.\n",
        "\n",
        "**Purpose:** Establish baseline performance before fine-tuning experiments.\n",
        "\n",
        "**Framework:** PyTorch + HuggingFace Diffusers\n",
        "\n",
        "**Model:** Stable Diffusion XL Base 1.0\n",
        "\n",
        "**Author:** Based on HuggingFace Diffusers library (https://github.com/huggingface/diffusers)\n",
        "\n",
        "**References:**\n",
        "- Podell et al., \"SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis\" (2023)\n",
        "- HuggingFace Diffusers Documentation: https://huggingface.co/docs/diffusers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "1. [Setup and Imports](#setup)\n",
        "2. [Configuration](#config)\n",
        "3. [Model Loading](#model)\n",
        "4. [Prompt Collection](#prompts)\n",
        "5. [Generation Pipeline](#generation)\n",
        "6. [Batch Generation](#batch)\n",
        "7. [Results and Evaluation](#results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Setup and Imports {#setup}\n",
        "\n",
        "Install required dependencies and import necessary libraries.\n",
        "\n",
        "**Prerequisites:**\n",
        "- Python 3.10+\n",
        "- CUDA-capable GPU (recommended 10GB+ VRAM)\n",
        "- Required packages listed in requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Dependencies installed and importable.\n"
          ]
        }
      ],
      "source": [
        "# Install required dependencies (run once per environment)\n",
        "# Note: You may need to restart the kernel after this cell finishes.\n",
        "\n",
        "%pip install -q --upgrade pip\n",
        "%pip install -q torch torchvision\n",
        "%pip install -q \"diffusers[torch]>=0.29.0\" \"transformers>=4.45.0\" \"accelerate>=0.34.0\" safetensors pillow tqdm\n",
        "\n",
        "# Quick import check\n",
        "try:\n",
        "    import diffusers, transformers, accelerate\n",
        "    print(\"Dependencies installed and importable.\")\n",
        "except Exception as e:\n",
        "    print(\"Dependency import check failed:\", e)\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.9.1\n",
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Numerical and image processing\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Deep learning frameworks\n",
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "from diffusers.utils import export_to_video\n",
        "\n",
        "# Progress tracking\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Configuration {#config}\n",
        "\n",
        "Define configuration parameters for generation.\n",
        "\n",
        "**Parameters:**\n",
        "- `num_inference_steps`: Number of denoising steps (higher = better quality, slower)\n",
        "- `guidance_scale`: How closely to follow the prompt (higher = more faithful to prompt)\n",
        "- `width`, `height`: Output image dimensions (SDXL supports up to 1024x1024)\n",
        "- `batch_size`: Number of images to generate per batch (adjust based on GPU memory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "  Model: stabilityai/stable-diffusion-xl-base-1.0\n",
            "  Device: cpu\n",
            "  Output directory: results/baseline\n",
            "  Seed: 42\n"
          ]
        }
      ],
      "source": [
        "# Generation parameters\n",
        "CONFIG = {\n",
        "    'model_id': 'stabilityai/stable-diffusion-xl-base-1.0',  # SDXL base model\n",
        "    'num_inference_steps': 50,  # Number of denoising steps (default: 50)\n",
        "    'guidance_scale': 7.5,  # Guidance scale (default: 7.5, range: 1-20)\n",
        "    'width': 1024,  # Output image width (SDXL max: 1024)\n",
        "    'height': 1024,  # Output image height (SDXL max: 1024)\n",
        "    'batch_size': 1,  # Images per batch (adjust based on GPU memory)\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',  # Use GPU if available\n",
        "    'dtype': torch.float16 if torch.cuda.is_available() else torch.float32,  # Use fp16 on GPU for speed\n",
        "}\n",
        "\n",
        "# Output directories\n",
        "OUTPUT_DIR = Path('results/baseline')\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "LOGS_DIR = Path('logs')\n",
        "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Reproducibility settings\n",
        "SEED = 42  # Default seed for reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Model: {CONFIG['model_id']}\")\n",
        "print(f\"  Device: {CONFIG['device']}\")\n",
        "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"  Seed: {SEED}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Model Loading {#model}\n",
        "\n",
        "Load the SDXL pipeline. This downloads the model weights on first run (~6.9GB).\n",
        "\n",
        "**Note:** Loading may take several minutes depending on internet speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading SDXL pipeline from stabilityai/stable-diffusion-xl-base-1.0...\n",
            "This may take several minutes on first run (model size ~6.9GB)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f5def02a5f340429242791ed6348304",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n",
            "CPU mode\n"
          ]
        }
      ],
      "source": [
        "# Load SDXL pipeline\n",
        "print(f\"Loading SDXL pipeline from {CONFIG['model_id']}...\")\n",
        "print(\"This may take several minutes on first run (model size ~6.9GB)...\")\n",
        "\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "    CONFIG['model_id'],\n",
        "    torch_dtype=CONFIG['dtype'],\n",
        "    use_safetensors=True,\n",
        "    variant=\"fp16\" if CONFIG['dtype'] == torch.float16 else None,\n",
        ")\n",
        "\n",
        "# Move to device\n",
        "pipe = pipe.to(CONFIG['device'])\n",
        "\n",
        "# Optimize memory usage (optional but recommended)\n",
        "pipe.enable_attention_slicing()  # Reduces VRAM usage at cost of slight speed\n",
        "# pipe.enable_model_cpu_offload()  # Alternative: offload model to CPU when not in use\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "print(f\"Memory usage: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\" if torch.cuda.is_available() else \"CPU mode\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Prompt Collection {#prompts}\n",
        "\n",
        "Define positive and negative prompts for speed bump generation.\n",
        "\n",
        "**Prompt Engineering Strategy:**\n",
        "- Positive prompts: Descriptive phrases that specify speed bump characteristics\n",
        "- Negative prompts: Terms to avoid unwanted artifacts (floating objects, distortions, etc.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total positive prompts: 23\n",
            "Sample prompts:\n",
            "  1. A speed bump on a road\n",
            "  2. A road with a speed bump\n",
            "  3. Speed bump in the middle of a street\n",
            "\n",
            "Negative prompt: blurry, distorted, deformed, disfigured, poorly drawn, bad anatomy, wrong anatomy, extra limb, missi...\n"
          ]
        }
      ],
      "source": [
        "# Positive prompts - Various phrasings for speed bumps\n",
        "POSITIVE_PROMPTS = [\n",
        "    # Basic descriptions\n",
        "    \"A speed bump on a road\",\n",
        "    \"A road with a speed bump\",\n",
        "    \"Speed bump in the middle of a street\",\n",
        "    \n",
        "    # Descriptive variations\n",
        "    \"A yellow-painted speed bump across a residential road\",\n",
        "    \"A black asphalt speed bump integrated into a paved street\",\n",
        "    \"A concrete speed hump on a neighborhood road\",\n",
        "    \"A speed cushion on a narrow residential street\",\n",
        "    \n",
        "    # Contextual variations\n",
        "    \"A speed bump in a parking lot with yellow markings\",\n",
        "    \"A road hump with reflective strips in a suburban area\",\n",
        "    \"A speed bump on a residential street during daytime\",\n",
        "    \"A traffic calming speed bump on a city road\",\n",
        "    \n",
        "    # Detailed descriptions\n",
        "    \"A raised speed bump made of rubber and asphalt, crossing a two-lane road\",\n",
        "    \"A painted speed bump with white stripes, smoothly integrated into the road surface\",\n",
        "    \"A speed bump with warning signs, on a paved street in good condition\",\n",
        "    \"A concrete speed hump with rounded edges, designed for traffic calming\",\n",
        "    \n",
        "    # Perspective variations\n",
        "    \"Front view of a speed bump on a road, photorealistic\",\n",
        "    \"Aerial view of a speed bump on a residential street\",\n",
        "    \"Side view of a speed bump integrated into asphalt road\",\n",
        "    \"Close-up of a speed bump with road texture details\",\n",
        "    \n",
        "    # Additional roadway fixtures\n",
        "    \"A speed table on a residential road\",\n",
        "    \"Multiple speed bumps in a parking lot\",\n",
        "    \"A raised crosswalk with speed bump characteristics\",\n",
        "    \"A speed bump near a school zone with markings\",\n",
        "]\n",
        "\n",
        "# Negative prompts - Terms to avoid unwanted artifacts\n",
        "NEGATIVE_PROMPT = (\n",
        "    \"blurry, distorted, deformed, disfigured, poorly drawn, bad anatomy, \"\n",
        "    \"wrong anatomy, extra limb, missing limb, floating limbs, disconnected limbs, \"\n",
        "    \"mutation, mutated, ugly, disgusting, amputation, floating, disconnected, \"\n",
        "    \"text, watermark, signature, out of focus, duplicate, morbid, mutilated, \"\n",
        "    \"extra fingers, mutated hands, poorly drawn hands, poorly drawn face, \"\n",
        "    \"artifacts, jpeg artifacts, compression artifacts, low quality, low resolution\"\n",
        ")\n",
        "\n",
        "print(f\"Total positive prompts: {len(POSITIVE_PROMPTS)}\")\n",
        "print(f\"Sample prompts:\")\n",
        "for i, prompt in enumerate(POSITIVE_PROMPTS[:3], 1):\n",
        "    print(f\"  {i}. {prompt}\")\n",
        "print(f\"\\nNegative prompt: {NEGATIVE_PROMPT[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Generation Pipeline {#generation}\n",
        "\n",
        "Define the core generation function.\n",
        "\n",
        "**Function:** `generate_image(prompt, negative_prompt, seed, **kwargs)`\n",
        "- Generates a single image from a text prompt\n",
        "- Returns PIL Image and generation metadata\n",
        "- Supports custom seeds for reproducibility\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generation function defined successfully!\n"
          ]
        }
      ],
      "source": [
        "def generate_image(\n",
        "    prompt: str,\n",
        "    negative_prompt: str = NEGATIVE_PROMPT,\n",
        "    seed: int = SEED,\n",
        "    num_inference_steps: int = None,\n",
        "    guidance_scale: float = None,\n",
        "    width: int = None,\n",
        "    height: int = None,\n",
        "    save_path: Optional[Path] = None\n",
        ") -> Tuple[Image.Image, Dict]:\n",
        "    \"\"\"\n",
        "    Generate a single image from a text prompt using SDXL.\n",
        "    \n",
        "    Args:\n",
        "        prompt: Positive text prompt describing the desired image\n",
        "        negative_prompt: Negative prompt to avoid unwanted features\n",
        "        seed: Random seed for reproducibility\n",
        "        num_inference_steps: Number of denoising steps (uses CONFIG default if None)\n",
        "        guidance_scale: Guidance scale (uses CONFIG default if None)\n",
        "        width: Output image width (uses CONFIG default if None)\n",
        "        height: Output image height (uses CONFIG default if None)\n",
        "        save_path: Optional path to save the image\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (PIL Image, metadata dictionary)\n",
        "    \"\"\"\n",
        "    # Set random seed for reproducibility\n",
        "    generator = torch.Generator(device=CONFIG['device']).manual_seed(seed)\n",
        "    \n",
        "    # Use config defaults if not specified\n",
        "    num_inference_steps = num_inference_steps or CONFIG['num_inference_steps']\n",
        "    guidance_scale = guidance_scale or CONFIG['guidance_scale']\n",
        "    width = width or CONFIG['width']\n",
        "    height = height or CONFIG['height']\n",
        "    \n",
        "    # Generate image with autocast for mixed precision\n",
        "    with autocast(CONFIG['device']):\n",
        "        result = pipe(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            width=width,\n",
        "            height=height,\n",
        "            generator=generator,\n",
        "        )\n",
        "    \n",
        "    image = result.images[0]\n",
        "    \n",
        "    # Create metadata\n",
        "    metadata = {\n",
        "        'prompt': prompt,\n",
        "        'negative_prompt': negative_prompt,\n",
        "        'seed': seed,\n",
        "        'num_inference_steps': num_inference_steps,\n",
        "        'guidance_scale': guidance_scale,\n",
        "        'width': width,\n",
        "        'height': height,\n",
        "        'model': CONFIG['model_id'],\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "    }\n",
        "    \n",
        "    # Save if path provided\n",
        "    if save_path:\n",
        "        image.save(save_path)\n",
        "        # Save metadata as JSON\n",
        "        metadata_path = save_path.with_suffix('.json')\n",
        "        with open(metadata_path, 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "    \n",
        "    return image, metadata\n",
        "\n",
        "print(\"Generation function defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch generation function defined successfully!\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "def generate_batch(\n",
        "    prompts: List[str],\n",
        "    negative_prompt: str = NEGATIVE_PROMPT,\n",
        "    base_seed: int = SEED,\n",
        "    use_varying_seeds: bool = True\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Generate images for a list of prompts with timing and progress tracking.\n",
        "    \n",
        "    Args:\n",
        "        prompts: List of positive prompts\n",
        "        negative_prompt: Negative prompt to use for all generations\n",
        "        base_seed: Base seed value (each prompt may use base_seed + index if varying)\n",
        "        use_varying_seeds: If True, use different seed for each prompt\n",
        "    \n",
        "    Returns:\n",
        "        List of metadata dictionaries for successful generations\n",
        "    \"\"\"\n",
        "    generation_log = []\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    start_time = time.time()\n",
        "    \n",
        "    print(f\"Generating {len(prompts)} images...\")\n",
        "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "    print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    generation_times = []\n",
        "    \n",
        "    for i, prompt in enumerate(tqdm(prompts, desc=\"Generating\")):\n",
        "        try:\n",
        "            gen_start = time.time()\n",
        "            \n",
        "            # Determine seed for this generation\n",
        "            seed = base_seed + i if use_varying_seeds else base_seed\n",
        "            \n",
        "            # Create filename: timestamp_promptID_seed.png\n",
        "            prompt_id = str(i).zfill(3)  # Zero-padded 3-digit ID\n",
        "            filename = f\"{timestamp}_{prompt_id}_{seed}.png\"\n",
        "            save_path = OUTPUT_DIR / filename\n",
        "            \n",
        "            # Generate image\n",
        "            image, metadata = generate_image(\n",
        "                prompt=prompt,\n",
        "                negative_prompt=negative_prompt,\n",
        "                seed=seed,\n",
        "                save_path=save_path\n",
        "            )\n",
        "            \n",
        "            gen_time = time.time() - gen_start\n",
        "            generation_times.append(gen_time)\n",
        "            \n",
        "            # Add file info to metadata\n",
        "            metadata['filename'] = filename\n",
        "            metadata['prompt_id'] = prompt_id\n",
        "            metadata['success'] = True\n",
        "            metadata['generation_time_seconds'] = round(gen_time, 2)\n",
        "            \n",
        "            generation_log.append(metadata)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"\\nError generating image for prompt {i}: {prompt}\")\n",
        "            print(f\"Error: {str(e)}\")\n",
        "            \n",
        "            # Log failure\n",
        "            generation_log.append({\n",
        "                'prompt_id': str(i).zfill(3),\n",
        "                'prompt': prompt,\n",
        "                'success': False,\n",
        "                'error': str(e),\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "            })\n",
        "            continue\n",
        "    \n",
        "    total_time = time.time() - start_time\n",
        "    successful = sum(1 for m in generation_log if m.get('success', False))\n",
        "    failed = sum(1 for m in generation_log if not m.get('success', True))\n",
        "    \n",
        "    # Calculate statistics\n",
        "    stats = {\n",
        "        'total_time_seconds': round(total_time, 2),\n",
        "        'total_time_hours': round(total_time / 3600, 2),\n",
        "        'average_time_per_image': round(np.mean(generation_times), 2) if generation_times else 0,\n",
        "        'min_time': round(min(generation_times), 2) if generation_times else 0,\n",
        "        'max_time': round(max(generation_times), 2) if generation_times else 0,\n",
        "    }\n",
        "    \n",
        "    # Save generation log\n",
        "    log_path = LOGS_DIR / f\"generation_log_{timestamp}.json\"\n",
        "    with open(log_path, 'w') as f:\n",
        "        json.dump({\n",
        "            'timestamp': timestamp,\n",
        "            'total_prompts': len(prompts),\n",
        "            'successful': successful,\n",
        "            'failed': failed,\n",
        "            'success_rate': round(successful / len(prompts) * 100, 2) if len(prompts) > 0 else 0,\n",
        "            'statistics': stats,\n",
        "            'config': CONFIG,\n",
        "            'generations': generation_log,\n",
        "        }, f, indent=2)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Batch generation complete!\")\n",
        "    print(f\"Successful: {successful}/{len(prompts)} ({successful/len(prompts)*100:.1f}%)\")\n",
        "    print(f\"Failed: {failed}\")\n",
        "    print(f\"\\nTiming Statistics:\")\n",
        "    print(f\"  Total time: {stats['total_time_seconds']}s ({stats['total_time_hours']:.2f} hours)\")\n",
        "    print(f\"  Average per image: {stats['average_time_per_image']}s\")\n",
        "    print(f\"  Min time: {stats['min_time']}s | Max time: {stats['max_time']}s\")\n",
        "    print(f\"\\nGeneration log saved to: {log_path}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    return generation_log\n",
        "\n",
        "print(\"Batch generation function defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Execute Batch Generation {#execute}\n",
        "\n",
        "Run the batch generation process.\n",
        "\n",
        "**Note:** This will generate images for all prompts in POSITIVE_PROMPTS.\n",
        "Expected runtime: ~[X] seconds per image on [GPU model].\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è∏Ô∏è  Batch generation is disabled. Set RUN_BATCH_GENERATION = True to execute.\n",
            "   This will generate 23 images.\n",
            "   Estimated time on CPU: ~ 115 - 230 minutes\n",
            "   Estimated time on GPU: ~ 345 - 690 seconds\n"
          ]
        }
      ],
      "source": [
        "# Execute batch generation\n",
        "# This will generate images for all prompts in POSITIVE_PROMPTS\n",
        "\n",
        "# Set to True to run batch generation, False to skip\n",
        "RUN_BATCH_GENERATION = False  # Change to True to execute\n",
        "\n",
        "if RUN_BATCH_GENERATION:\n",
        "    generation_log = generate_batch(\n",
        "        prompts=POSITIVE_PROMPTS,\n",
        "        negative_prompt=NEGATIVE_PROMPT,\n",
        "        base_seed=SEED,\n",
        "        use_varying_seeds=True\n",
        "    )\n",
        "    print(\"\\n‚úÖ Batch generation completed! Check results/baseline/ for generated images.\")\n",
        "else:\n",
        "    print(\"‚è∏Ô∏è  Batch generation is disabled. Set RUN_BATCH_GENERATION = True to execute.\")\n",
        "    print(\"   This will generate\", len(POSITIVE_PROMPTS), \"images.\")\n",
        "    print(\"   Estimated time on CPU: ~\", len(POSITIVE_PROMPTS) * 5, \"-\", len(POSITIVE_PROMPTS) * 10, \"minutes\")\n",
        "    print(\"   Estimated time on GPU: ~\", len(POSITIVE_PROMPTS) * 15, \"-\", len(POSITIVE_PROMPTS) * 30, \"seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. Test Single Generation {#test}\n",
        "\n",
        "Optional: Test generation with a single prompt before running full batch.\n",
        "\n",
        "Useful for:\n",
        "- Verifying pipeline is working correctly\n",
        "- Testing prompt variations\n",
        "- Quick iteration on prompt engineering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è∏Ô∏è  Test generation is disabled. Set RUN_TEST_GENERATION = True to execute.\n",
            "   This will generate 1 test image to verify the pipeline is working.\n"
          ]
        }
      ],
      "source": [
        "# Test single generation\n",
        "# Set to True to run a test generation, False to skip\n",
        "RUN_TEST_GENERATION = False  # Change to True to execute\n",
        "\n",
        "if RUN_TEST_GENERATION:\n",
        "    test_prompt = POSITIVE_PROMPTS[0]\n",
        "    print(f\"Testing with prompt: {test_prompt}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    test_image, test_metadata = generate_image(\n",
        "        prompt=test_prompt,\n",
        "        negative_prompt=NEGATIVE_PROMPT,\n",
        "        seed=42\n",
        "    )\n",
        "    \n",
        "    # Display image\n",
        "    print(f\"\\n‚úÖ Test image generated successfully!\")\n",
        "    print(f\"Image size: {test_image.size}\")\n",
        "    display(test_image)\n",
        "    \n",
        "    print(f\"\\nMetadata:\")\n",
        "    print(json.dumps(test_metadata, indent=2))\n",
        "else:\n",
        "    print(\"‚è∏Ô∏è  Test generation is disabled. Set RUN_TEST_GENERATION = True to execute.\")\n",
        "    print(\"   This will generate 1 test image to verify the pipeline is working.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 9. Results Summary {#results}\n",
        "\n",
        "View summary statistics from generation log.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ÑπÔ∏è  No generation logs found.\n",
            "   Run batch generation first to create a log file.\n"
          ]
        }
      ],
      "source": [
        "# Load and display generation log summary\n",
        "import glob\n",
        "\n",
        "log_files = sorted(glob.glob(str(LOGS_DIR / 'generation_log_*.json')))\n",
        "\n",
        "if log_files:\n",
        "    latest_log = log_files[-1]\n",
        "    with open(latest_log, 'r') as f:\n",
        "        log_data = json.load(f)\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"GENERATION RESULTS SUMMARY\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Latest generation log: {latest_log}\")\n",
        "    print(f\"\\nOverall Statistics:\")\n",
        "    print(f\"  Total prompts: {log_data['total_prompts']}\")\n",
        "    print(f\"  Successful: {log_data['successful']}\")\n",
        "    print(f\"  Failed: {log_data['failed']}\")\n",
        "    print(f\"  Success rate: {log_data.get('success_rate', log_data['successful']/log_data['total_prompts']*100):.1f}%\")\n",
        "    \n",
        "    if 'statistics' in log_data:\n",
        "        stats = log_data['statistics']\n",
        "        print(f\"\\nTiming Statistics:\")\n",
        "        print(f\"  Total time: {stats.get('total_time_seconds', 'N/A')}s ({stats.get('total_time_hours', 'N/A'):.2f} hours)\")\n",
        "        print(f\"  Average per image: {stats.get('average_time_per_image', 'N/A')}s\")\n",
        "        print(f\"  Min time: {stats.get('min_time', 'N/A')}s\")\n",
        "        print(f\"  Max time: {stats.get('max_time', 'N/A')}s\")\n",
        "    \n",
        "    print(f\"\\nConfiguration:\")\n",
        "    config = log_data.get('config', {})\n",
        "    print(f\"  Model: {config.get('model_id', 'N/A')}\")\n",
        "    print(f\"  Device: {config.get('device', 'N/A')}\")\n",
        "    print(f\"  Inference steps: {config.get('num_inference_steps', 'N/A')}\")\n",
        "    print(f\"  Guidance scale: {config.get('guidance_scale', 'N/A')}\")\n",
        "    print(f\"  Image size: {config.get('width', 'N/A')}x{config.get('height', 'N/A')}\")\n",
        "    print(\"=\" * 60)\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è  No generation logs found.\")\n",
        "    print(\"   Run batch generation first to create a log file.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 10. Visualization and Image Gallery {#visualization}\n",
        "\n",
        "Display generated images in a gallery format for review.\n",
        "\n",
        "**Features:**\n",
        "- Load all generated images from results directory\n",
        "- Display images in a grid layout\n",
        "- Show prompt information with each image\n",
        "- Filter and view specific generations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Image gallery function ready!\n",
            "   Uncomment the display_image_gallery() call above to view generated images.\n"
          ]
        }
      ],
      "source": [
        "# Visualization: Display generated images\n",
        "import glob\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def display_image_gallery(images_dir: Path, max_images: int = 9):\n",
        "    \"\"\"\n",
        "    Display a gallery of generated images.\n",
        "    \n",
        "    Args:\n",
        "        images_dir: Directory containing generated images\n",
        "        max_images: Maximum number of images to display\n",
        "    \"\"\"\n",
        "    image_files = sorted(glob.glob(str(images_dir / '*.png')))\n",
        "    \n",
        "    if not image_files:\n",
        "        print(\"No images found in\", images_dir)\n",
        "        return\n",
        "    \n",
        "    print(f\"Found {len(image_files)} images. Displaying first {min(max_images, len(image_files))} images.\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    # Display images in a grid\n",
        "    images_to_show = image_files[:max_images]\n",
        "    \n",
        "    for img_path in images_to_show:\n",
        "        # Load image\n",
        "        img = Image.open(img_path)\n",
        "        \n",
        "        # Try to load metadata\n",
        "        metadata_path = Path(img_path).with_suffix('.json')\n",
        "        prompt = \"Unknown\"\n",
        "        if metadata_path.exists():\n",
        "            with open(metadata_path, 'r') as f:\n",
        "                metadata = json.load(f)\n",
        "                prompt = metadata.get('prompt', 'Unknown')\n",
        "        \n",
        "        # Display\n",
        "        print(f\"\\nüì∑ {Path(img_path).name}\")\n",
        "        print(f\"   Prompt: {prompt[:80]}...\" if len(prompt) > 80 else f\"   Prompt: {prompt}\")\n",
        "        display(img)\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "# Display gallery (uncomment to view)\n",
        "# display_image_gallery(OUTPUT_DIR, max_images=9)\n",
        "\n",
        "print(\"‚úÖ Image gallery function ready!\")\n",
        "print(\"   Uncomment the display_image_gallery() call above to view generated images.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 11. Detailed Analysis {#analysis}\n",
        "\n",
        "Analyze generation results in detail.\n",
        "\n",
        "**Analysis includes:**\n",
        "- Prompt performance statistics\n",
        "- Generation time analysis\n",
        "- File organization verification\n",
        "- Prompt category breakdown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Analysis function ready!\n",
            "   Uncomment the analyze_generation_results() call above to run analysis.\n"
          ]
        }
      ],
      "source": [
        "# Detailed analysis of generation results\n",
        "def analyze_generation_results(log_path: Path = None):\n",
        "    \"\"\"\n",
        "    Perform detailed analysis of generation results.\n",
        "    \n",
        "    Args:\n",
        "        log_path: Path to generation log JSON file (uses latest if None)\n",
        "    \"\"\"\n",
        "    if log_path is None:\n",
        "        log_files = sorted(glob.glob(str(LOGS_DIR / 'generation_log_*.json')))\n",
        "        if not log_files:\n",
        "            print(\"‚ùå No generation logs found. Run batch generation first.\")\n",
        "            return\n",
        "        log_path = Path(log_files[-1])\n",
        "    \n",
        "    with open(log_path, 'r') as f:\n",
        "        log_data = json.load(f)\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"DETAILED ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Load all successful generations\n",
        "    successful_gens = [g for g in log_data['generations'] if g.get('success', False)]\n",
        "    failed_gens = [g for g in log_data['generations'] if not g.get('success', True)]\n",
        "    \n",
        "    print(f\"\\n1. Generation Statistics:\")\n",
        "    print(f\"   Total: {log_data['total_prompts']}\")\n",
        "    print(f\"   Successful: {len(successful_gens)} ({len(successful_gens)/log_data['total_prompts']*100:.1f}%)\")\n",
        "    print(f\"   Failed: {len(failed_gens)} ({len(failed_gens)/log_data['total_prompts']*100:.1f}%)\")\n",
        "    \n",
        "    # Timing analysis\n",
        "    if successful_gens and 'generation_time_seconds' in successful_gens[0]:\n",
        "        times = [g['generation_time_seconds'] for g in successful_gens if 'generation_time_seconds' in g]\n",
        "        if times:\n",
        "            print(f\"\\n2. Timing Analysis:\")\n",
        "            print(f\"   Average: {np.mean(times):.2f}s\")\n",
        "            print(f\"   Median: {np.median(times):.2f}s\")\n",
        "            print(f\"   Min: {min(times):.2f}s\")\n",
        "            print(f\"   Max: {max(times):.2f}s\")\n",
        "            print(f\"   Std Dev: {np.std(times):.2f}s\")\n",
        "    \n",
        "    # Prompt analysis\n",
        "    print(f\"\\n3. Prompt Categories:\")\n",
        "    prompt_categories = {\n",
        "        'Basic': [\"A speed bump\", \"A road with\", \"Speed bump in\"],\n",
        "        'Descriptive': [\"yellow-painted\", \"black asphalt\", \"concrete\", \"painted\"],\n",
        "        'Contextual': [\"parking lot\", \"residential\", \"school zone\", \"suburban\"],\n",
        "        'Perspective': [\"Front view\", \"Aerial view\", \"Side view\", \"Close-up\"],\n",
        "    }\n",
        "    \n",
        "    for category, keywords in prompt_categories.items():\n",
        "        matching = [g for g in successful_gens \n",
        "                   if any(kw.lower() in g.get('prompt', '').lower() for kw in keywords)]\n",
        "        print(f\"   {category}: {len(matching)} images\")\n",
        "    \n",
        "    # File verification\n",
        "    print(f\"\\n4. File Verification:\")\n",
        "    image_files = glob.glob(str(OUTPUT_DIR / '*.png'))\n",
        "    json_files = glob.glob(str(OUTPUT_DIR / '*.json'))\n",
        "    print(f\"   PNG files: {len(image_files)}\")\n",
        "    print(f\"   JSON metadata files: {len(json_files)}\")\n",
        "    \n",
        "    if len(successful_gens) != len(image_files):\n",
        "        print(f\"   ‚ö†Ô∏è  Warning: Mismatch between successful generations ({len(successful_gens)}) and PNG files ({len(image_files)})\")\n",
        "    \n",
        "    # Failed generations\n",
        "    if failed_gens:\n",
        "        print(f\"\\n5. Failed Generations:\")\n",
        "        for fail in failed_gens[:5]:  # Show first 5 failures\n",
        "            print(f\"   Prompt ID: {fail.get('prompt_id', 'N/A')}\")\n",
        "            print(f\"   Error: {fail.get('error', 'Unknown')[:60]}...\")\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "\n",
        "# Run analysis (uncomment to execute)\n",
        "# analyze_generation_results()\n",
        "\n",
        "print(\"‚úÖ Analysis function ready!\")\n",
        "print(\"   Uncomment the analyze_generation_results() call above to run analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Notes and Troubleshooting\n",
        "\n",
        "### Memory Issues\n",
        "- If you encounter CUDA out of memory errors, try:\n",
        "  - Reducing batch_size in CONFIG\n",
        "  - Reducing image dimensions (width, height)\n",
        "  - Enabling `enable_model_cpu_offload()` instead of `enable_attention_slicing()`\n",
        "\n",
        "### Reproducibility\n",
        "- Same seed + same prompt + same parameters = same output\n",
        "- Use `use_varying_seeds=False` in `generate_batch()` to test reproducibility\n",
        "\n",
        "### Output Organization\n",
        "- Generated images: `results/baseline/`\n",
        "- Metadata JSON files: `results/baseline/` (alongside images)\n",
        "- Generation logs: `logs/generation_log_*.json`\n",
        "\n",
        "### Performance\n",
        "- Typical generation time: ~10-20 seconds per image on modern GPU\n",
        "- CPU generation is much slower (~5-10 minutes per image)\n",
        "- Consider batch size and generation count when estimating total time\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
