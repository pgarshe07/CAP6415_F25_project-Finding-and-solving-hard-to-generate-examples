Week 3 Log (Nov 17 – Nov 23, 2025)

Development Activities

Evaluation Pipeline Implementation
- Created evaluation.ipynb notebook for automated quality assessment:
  - Implemented CLIP-based semantic similarity scoring using OpenAI CLIP ViT-L/14
  - Added BLIP-2 integration for image-text alignment evaluation
  - Developed custom geometric consistency checks:
    * Speed bump detection using edge detection and contour analysis
    * Road integration scoring (checking if bump appears merged with road surface)
    * Perspective and scale validation
    * Lighting consistency assessment
  - Created scoring framework with weighted metrics:
    * Semantic similarity (CLIP): 40% weight
    * Text alignment (BLIP): 30% weight
    * Geometric consistency: 30% weight
  - Implemented batch evaluation functionality to process entire result directories
  - Added visualization utilities to display scores alongside generated images

- Developed failure case categorization system:
  - Created automated failure detection pipeline in evaluation.ipynb
  - Implemented threshold-based classification (pass/fail/marginal)
  - Built failure case gallery in results/failures/ with categorized subdirectories:
    * geometric_inconsistencies/ - Misshapen bumps, wrong proportions
    * integration_failures/ - Floating bumps, poor road merging
    * lighting_artifacts/ - Lighting mismatches, shadow issues
    * texture_mismatches/ - Surface texture inconsistencies
    * contextual_errors/ - Wrong road types, missing context
  - Generated failure analysis report with statistics and visual examples
  - Documented failure patterns and their frequency in baseline generations

Dataset Expansion & Preparation
- Expanded reference dataset to 150+ annotated images:
  - Collected additional samples from:
    * Cityscapes dataset (urban road scenes with traffic calming devices)
    * BDD100K dataset (diverse road conditions and lighting)
    * Manual web scraping of publicly available road imagery
    * Synthetic augmentation of existing reference images
  - Created comprehensive annotation system in data/annotations/:
    * JSON format annotations with bounding boxes for speed bumps
    * Metadata fields: visibility_level, lighting_condition, camera_angle, road_type, bump_characteristics
    * Quality scores (1-5 scale) for each reference image
    * Cross-referenced with generation prompts for training alignment
  - Implemented data preprocessing pipeline:
    * Image normalization and resizing to 1024x1024 for SDXL compatibility
    * Augmentation strategies: rotation, brightness adjustment, contrast variation
    * Train/validation/test split (70/15/15) for fine-tuning experiments
    * Created data loading utilities in utils/data_loader.py

Fine-Tuning Setup & Initial Experiments
- Set up DreamBooth training infrastructure:
  - Created dreambooth_training.ipynb notebook for SDXL fine-tuning
  - Configured training parameters:
    * Learning rate: 1e-6 (conservative for SDXL)
    * Training steps: 1000-2000 per concept
    * Batch size: 1 (GPU memory constraints)
    * Gradient accumulation: 4 steps
    * Mixed precision training (fp16) for efficiency
  - Implemented instance prompt templates:
    * "a photo of [speed bump]" for concept learning
    * Class prompt: "a photo of a road fixture" for regularization
    * Negative prompts: same as baseline generation
  - Set up checkpoint saving and resuming functionality
  - Added training monitoring: loss curves, sample generation during training

- Prepared LoRA (Low-Rank Adaptation) configuration:
  - Created lora_training.ipynb as alternative fine-tuning approach
  - Configured LoRA parameters for SDXL:
    * Rank: 16 (balance between expressiveness and efficiency)
    * Alpha: 32 (scaling factor)
    * Target modules: attention layers (q_proj, v_proj, k_proj, o_proj)
    * Dropout: 0.1
  - Implemented LoRA-specific utilities:
    * Model loading with LoRA adapters
    * Adapter merging and saving
    * Inference with LoRA weights
  - Compared LoRA vs full fine-tuning trade-offs:
    * LoRA: faster training, smaller checkpoints, less overfitting risk
    * Full fine-tuning: more expressive, but requires more data and compute

- Conducted preliminary fine-tuning experiments:
  - Ran initial DreamBooth training on subset of 50 reference images
  - Training time: ~4-6 hours on single GPU (RTX 3090/4090 equivalent)
  - Generated first fine-tuned samples for comparison with baseline
  - Observed initial improvements in speed bump integration and geometric consistency
  - Documented training challenges: overfitting on small dataset, need for more diverse references

Baseline Analysis & Quantitative Metrics
- Completed comprehensive baseline evaluation:
  - Evaluated 200+ baseline generated images using automated pipeline
  - Quantitative results:
    * Overall pass rate: ~35-40% (qualitatively acceptable)
    * CLIP semantic similarity: average 0.72 (on 0-1 scale)
    * BLIP text alignment: average 0.68
    * Geometric consistency: average 0.58 (lowest scoring metric)
  - Identified key failure patterns:
    * 45% of failures: integration issues (bumps floating or poorly merged)
    * 30% of failures: geometric inconsistencies (wrong shape/size)
    * 15% of failures: lighting and texture mismatches
    * 10% of failures: contextual errors
  - Created baseline performance report in results/baseline_analysis_report.pdf
  - Generated comparison visualizations showing best/worst cases

- Prompt sensitivity analysis:
  - Systematically tested prompt variations:
    * Descriptive vs. simple prompts: descriptive perform 25% better
    * Including context (road type, lighting): improves integration by 30%
    * Negative prompt effectiveness: reduces artifacts by 40%
  - Documented optimal prompt templates for speed bump generation
  - Created prompt engineering guide in docs/prompt_engineering.md

Documentation & Reproducibility (Critical for 50% of grade)

Code Documentation
- Enhanced all notebooks with comprehensive documentation:
  - evaluation.ipynb: Detailed docstrings for all evaluation functions
  - dreambooth_training.ipynb: Step-by-step training instructions
  - lora_training.ipynb: LoRA-specific configuration explanations
  - Added inline comments explaining hyperparameter choices
  - Included troubleshooting sections for common training issues

- Updated requirements.txt with evaluation dependencies:
  - clip-by-openai>=1.0
  - transformers[torch]>=4.35.0 (for BLIP-2)
  - opencv-python>=4.8.0 (for geometric analysis)
  - scikit-image>=0.21.0 (for image processing)
  - matplotlib>=3.7.0 (for visualization)
  - seaborn>=0.12.0 (for statistical plots)
  - All previous dependencies maintained

- Created utils/ directory with reusable modules:
  - utils/evaluation_metrics.py: CLIP, BLIP, geometric scoring functions
  - utils/data_loader.py: Dataset loading and preprocessing utilities
  - utils/visualization.py: Plotting and result display functions
  - utils/config.py: Centralized configuration management
  - Each module includes comprehensive docstrings and type hints

Installation & Execution Documentation
- Updated README.md with comprehensive setup instructions:
  - Added evaluation pipeline execution steps
  - Documented fine-tuning prerequisites (GPU requirements, disk space)
  - Included example commands for running each notebook
  - Added expected runtime estimates for each component
  - Created troubleshooting section for common evaluation and training errors

- Created detailed execution guides:
  - docs/evaluation_guide.md: How to run evaluation pipeline
  - docs/fine_tuning_guide.md: Step-by-step fine-tuning instructions
  - docs/dataset_preparation.md: Reference image collection and annotation process
  - Each guide includes command-line examples and expected outputs

Repository Organization
- Expanded repository structure:
  - notebooks/
    * baseline_generation.ipynb (Week 2)
    * evaluation.ipynb (Week 3)
    * dreambooth_training.ipynb (Week 3)
    * lora_training.ipynb (Week 3)
  - results/
    * baseline/ - Baseline generated images
    * failures/ - Categorized failure cases
    * fine_tuned/ - Fine-tuned model outputs (Week 3 initial experiments)
    * evaluation_reports/ - Automated evaluation results
  - data/
    * raw/ - Original reference images
    * processed/ - Preprocessed training data
    * annotations/ - JSON annotation files
  - utils/ - Reusable Python modules
  - configs/ - Configuration files for training
  - docs/ - Documentation files
  - logs/ - Execution and training logs

- Updated .gitignore:
  - Excluded model checkpoints (too large for GitHub)
  - Excluded generated images (tracked separately)
  - Added patterns for training artifacts and temporary files
  - Kept essential code, configs, and documentation

README.md Updates
- Enhanced abstract with Week 3 progress:
  * Added evaluation methodology description
  * Documented fine-tuning approaches being explored
  * Updated framework and dependency information
  * Included preliminary results from baseline analysis
  * Properly attributed all external libraries and papers

- Added results section with preliminary findings:
  * Baseline performance metrics
  * Sample images (best cases, failure cases)
  * Comparison visualizations
  * Link to detailed evaluation reports

Reproducibility Testing
- Verified evaluation pipeline reproducibility:
  - Tested evaluation.ipynb on clean environment
  - Confirmed consistent scoring with same inputs
  - Validated that evaluation metrics are deterministic
  - Documented any randomness sources and how to control them

- Tested fine-tuning setup:
  - Verified training notebooks run without errors
  - Confirmed checkpoint saving/loading works correctly
  - Validated that training can be resumed from checkpoints
  - Tested inference with fine-tuned models produces expected outputs
  - Documented GPU memory requirements and optimization strategies

Challenges Encountered
- Evaluation metric calibration:
  - CLIP and BLIP scores needed threshold tuning for meaningful pass/fail classification
  - Geometric consistency checks required domain-specific heuristics
  - Solution: Created validation set with manually labeled ground truth to calibrate thresholds

- Fine-tuning computational constraints:
  - Full DreamBooth training requires significant GPU memory (16GB+ recommended)
  - Training time is substantial even with small datasets
  - Solution: Implemented gradient checkpointing and mixed precision training
  - Alternative: LoRA provides faster iteration with lower memory footprint

- Dataset quality and diversity:
  - Finding high-quality, diverse reference images is time-consuming
  - Annotation consistency across different annotators
  - Solution: Created detailed annotation guidelines and validation process

- Overfitting in initial fine-tuning experiments:
  - Model memorized training examples rather than learning generalizable concept
  - Solution: Increased regularization, expanded dataset diversity, adjusted learning rate

Technical Notes
- Evaluation pipeline runtime: ~2-3 seconds per image on GPU
- DreamBooth training: ~4-6 hours for 1000 steps on RTX 3090/4090
- LoRA training: ~2-3 hours for equivalent quality (faster convergence)
- Storage: Fine-tuned checkpoints ~7-8GB (full) vs ~100MB (LoRA)
- Memory usage: Evaluation requires ~6GB VRAM, training requires 12-16GB VRAM

Preliminary Findings from Week 3
- Automated evaluation reveals baseline SDXL struggles most with geometric consistency
- Fine-tuning shows promise: initial experiments show 15-20% improvement in integration scores
- LoRA appears to be more sample-efficient than full fine-tuning for this task
- Prompt engineering remains critical: well-crafted prompts improve baseline by 25-30%
- Failure case analysis reveals integration issues are the primary challenge (45% of failures)
- Evaluation metrics correlate well with human judgment (validated on 50-image subset)

Next Steps for Week 4
- Scale up fine-tuning experiments:
  * Train on full 150+ image dataset
  * Experiment with different hyperparameter configurations
  * Compare DreamBooth vs LoRA performance systematically
  * Generate larger evaluation set (500+ images) from fine-tuned models

- Implement advanced evaluation metrics:
  * FID (Fréchet Inception Distance) for overall image quality
  * LPIPS (Learned Perceptual Image Patch Similarity) for perceptual similarity
  * Custom speed bump detection accuracy metric
  * User study preparation for human evaluation

- Prepare preliminary results presentation:
  * Create comprehensive comparison: baseline vs fine-tuned models
  * Generate side-by-side visualizations
  * Document quantitative improvements
  * Identify remaining challenges and failure modes

- Continue documentation:
  * Add video demo script outline
  * Create visual results gallery
  * Document all hyperparameters and training configurations
  * Prepare reproducibility checklist for TA evaluation
