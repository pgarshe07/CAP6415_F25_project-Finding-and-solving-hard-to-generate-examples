{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline Generation Notebook\n",
        "\n",
        "## Project: Finding and Solving Hard-to-Generate Examples - Speed Bumps\n",
        "\n",
        "This notebook implements the baseline image generation pipeline using Stable Diffusion XL (SDXL)\n",
        "to generate images of speed bumps and related roadway fixtures.\n",
        "\n",
        "**Purpose:** Establish baseline performance before fine-tuning experiments.\n",
        "\n",
        "**Framework:** PyTorch + HuggingFace Diffusers\n",
        "\n",
        "**Model:** Stable Diffusion XL Base 1.0\n",
        "\n",
        "**Author:** Based on HuggingFace Diffusers library (https://github.com/huggingface/diffusers)\n",
        "\n",
        "**References:**\n",
        "- Podell et al., \"SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis\" (2023)\n",
        "- HuggingFace Diffusers Documentation: https://huggingface.co/docs/diffusers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "1. [Setup and Imports](#setup)\n",
        "2. [Configuration](#config)\n",
        "3. [Model Loading](#model)\n",
        "4. [Prompt Collection](#prompts)\n",
        "5. [Generation Pipeline](#generation)\n",
        "6. [Batch Generation](#batch)\n",
        "7. [Results and Evaluation](#results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Setup and Imports {#setup}\n",
        "\n",
        "Install required dependencies and import necessary libraries.\n",
        "\n",
        "**Prerequisites:**\n",
        "- Python 3.10+\n",
        "- CUDA-capable GPU (recommended 10GB+ VRAM)\n",
        "- Required packages listed in requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Dependencies installed and importable.\n"
          ]
        }
      ],
      "source": [
        "# Install required dependencies (run once per environment)\n",
        "# Note: You may need to restart the kernel after this cell finishes.\n",
        "\n",
        "%pip install -q --upgrade pip\n",
        "%pip install -q torch torchvision\n",
        "%pip install -q \"diffusers[torch]>=0.29.0\" \"transformers>=4.45.0\" \"accelerate>=0.34.0\" safetensors pillow tqdm\n",
        "\n",
        "# Quick import check\n",
        "try:\n",
        "    import diffusers, transformers, accelerate\n",
        "    print(\"Dependencies installed and importable.\")\n",
        "except Exception as e:\n",
        "    print(\"Dependency import check failed:\", e)\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
            "PyTorch version: 2.9.1\n",
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Numerical and image processing\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Deep learning frameworks\n",
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "from diffusers.utils import export_to_video\n",
        "\n",
        "# Progress tracking\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Configuration {#config}\n",
        "\n",
        "Define configuration parameters for generation.\n",
        "\n",
        "**Parameters:**\n",
        "- `num_inference_steps`: Number of denoising steps (higher = better quality, slower)\n",
        "- `guidance_scale`: How closely to follow the prompt (higher = more faithful to prompt)\n",
        "- `width`, `height`: Output image dimensions (SDXL supports up to 1024x1024)\n",
        "- `batch_size`: Number of images to generate per batch (adjust based on GPU memory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "  Model: stabilityai/stable-diffusion-xl-base-1.0\n",
            "  Device: cpu\n",
            "  Output directory: results/baseline\n",
            "  Seed: 42\n"
          ]
        }
      ],
      "source": [
        "# Generation parameters\n",
        "CONFIG = {\n",
        "    'model_id': 'stabilityai/stable-diffusion-xl-base-1.0',  # SDXL base model\n",
        "    'num_inference_steps': 50,  # Number of denoising steps (default: 50)\n",
        "    'guidance_scale': 7.5,  # Guidance scale (default: 7.5, range: 1-20)\n",
        "    'width': 1024,  # Output image width (SDXL max: 1024)\n",
        "    'height': 1024,  # Output image height (SDXL max: 1024)\n",
        "    'batch_size': 1,  # Images per batch (adjust based on GPU memory)\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',  # Use GPU if available\n",
        "    'dtype': torch.float16 if torch.cuda.is_available() else torch.float32,  # Use fp16 on GPU for speed\n",
        "}\n",
        "\n",
        "# Output directories\n",
        "OUTPUT_DIR = Path('results/baseline')\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "LOGS_DIR = Path('logs')\n",
        "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Reproducibility settings\n",
        "SEED = 42  # Default seed for reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Model: {CONFIG['model_id']}\")\n",
        "print(f\"  Device: {CONFIG['device']}\")\n",
        "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"  Seed: {SEED}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Model Loading {#model}\n",
        "\n",
        "Load the SDXL pipeline. This downloads the model weights on first run (~6.9GB).\n",
        "\n",
        "**Note:** Loading may take several minutes depending on internet speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading SDXL pipeline from stabilityai/stable-diffusion-xl-base-1.0...\n",
            "This may take several minutes on first run (model size ~6.9GB)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29c5a8a007f74468ae3ca2b1c8f3f821",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n",
            "CPU mode\n"
          ]
        }
      ],
      "source": [
        "# Load SDXL pipeline\n",
        "print(f\"Loading SDXL pipeline from {CONFIG['model_id']}...\")\n",
        "print(\"This may take several minutes on first run (model size ~6.9GB)...\")\n",
        "\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "    CONFIG['model_id'],\n",
        "    torch_dtype=CONFIG['dtype'],\n",
        "    use_safetensors=True,\n",
        "    variant=\"fp16\" if CONFIG['dtype'] == torch.float16 else None,\n",
        ")\n",
        "\n",
        "# Move to device\n",
        "pipe = pipe.to(CONFIG['device'])\n",
        "\n",
        "# Optimize memory usage (optional but recommended)\n",
        "pipe.enable_attention_slicing()  # Reduces VRAM usage at cost of slight speed\n",
        "# pipe.enable_model_cpu_offload()  # Alternative: offload model to CPU when not in use\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "print(f\"Memory usage: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\" if torch.cuda.is_available() else \"CPU mode\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Prompt Collection {#prompts}\n",
        "\n",
        "Define positive and negative prompts for speed bump generation.\n",
        "\n",
        "**Prompt Engineering Strategy:**\n",
        "- Positive prompts: Descriptive phrases that specify speed bump characteristics\n",
        "- Negative prompts: Terms to avoid unwanted artifacts (floating objects, distortions, etc.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total positive prompts: 23\n",
            "Sample prompts:\n",
            "  1. A speed bump on a road\n",
            "  2. A road with a speed bump\n",
            "  3. Speed bump in the middle of a street\n",
            "\n",
            "Negative prompt: blurry, distorted, deformed, disfigured, poorly drawn, bad anatomy, wrong anatomy, extra limb, missi...\n"
          ]
        }
      ],
      "source": [
        "# Positive prompts - Various phrasings for speed bumps\n",
        "POSITIVE_PROMPTS = [\n",
        "    # Basic descriptions\n",
        "    \"A speed bump on a road\",\n",
        "    \"A road with a speed bump\",\n",
        "    \"Speed bump in the middle of a street\",\n",
        "    \n",
        "    # Descriptive variations\n",
        "    \"A yellow-painted speed bump across a residential road\",\n",
        "    \"A black asphalt speed bump integrated into a paved street\",\n",
        "    \"A concrete speed hump on a neighborhood road\",\n",
        "    \"A speed cushion on a narrow residential street\",\n",
        "    \n",
        "    # Contextual variations\n",
        "    \"A speed bump in a parking lot with yellow markings\",\n",
        "    \"A road hump with reflective strips in a suburban area\",\n",
        "    \"A speed bump on a residential street during daytime\",\n",
        "    \"A traffic calming speed bump on a city road\",\n",
        "    \n",
        "    # Detailed descriptions\n",
        "    \"A raised speed bump made of rubber and asphalt, crossing a two-lane road\",\n",
        "    \"A painted speed bump with white stripes, smoothly integrated into the road surface\",\n",
        "    \"A speed bump with warning signs, on a paved street in good condition\",\n",
        "    \"A concrete speed hump with rounded edges, designed for traffic calming\",\n",
        "    \n",
        "    # Perspective variations\n",
        "    \"Front view of a speed bump on a road, photorealistic\",\n",
        "    \"Aerial view of a speed bump on a residential street\",\n",
        "    \"Side view of a speed bump integrated into asphalt road\",\n",
        "    \"Close-up of a speed bump with road texture details\",\n",
        "    \n",
        "    # Additional roadway fixtures\n",
        "    \"A speed table on a residential road\",\n",
        "    \"Multiple speed bumps in a parking lot\",\n",
        "    \"A raised crosswalk with speed bump characteristics\",\n",
        "    \"A speed bump near a school zone with markings\",\n",
        "]\n",
        "\n",
        "# Negative prompts - Terms to avoid unwanted artifacts\n",
        "NEGATIVE_PROMPT = (\n",
        "    \"blurry, distorted, deformed, disfigured, poorly drawn, bad anatomy, \"\n",
        "    \"wrong anatomy, extra limb, missing limb, floating limbs, disconnected limbs, \"\n",
        "    \"mutation, mutated, ugly, disgusting, amputation, floating, disconnected, \"\n",
        "    \"text, watermark, signature, out of focus, duplicate, morbid, mutilated, \"\n",
        "    \"extra fingers, mutated hands, poorly drawn hands, poorly drawn face, \"\n",
        "    \"artifacts, jpeg artifacts, compression artifacts, low quality, low resolution\"\n",
        ")\n",
        "\n",
        "print(f\"Total positive prompts: {len(POSITIVE_PROMPTS)}\")\n",
        "print(f\"Sample prompts:\")\n",
        "for i, prompt in enumerate(POSITIVE_PROMPTS[:3], 1):\n",
        "    print(f\"  {i}. {prompt}\")\n",
        "print(f\"\\nNegative prompt: {NEGATIVE_PROMPT[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Generation Pipeline {#generation}\n",
        "\n",
        "Define the core generation function.\n",
        "\n",
        "**Function:** `generate_image(prompt, negative_prompt, seed, **kwargs)`\n",
        "- Generates a single image from a text prompt\n",
        "- Returns PIL Image and generation metadata\n",
        "- Supports custom seeds for reproducibility\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generation function defined successfully!\n"
          ]
        }
      ],
      "source": [
        "def generate_image(\n",
        "    prompt: str,\n",
        "    negative_prompt: str = NEGATIVE_PROMPT,\n",
        "    seed: int = SEED,\n",
        "    num_inference_steps: int = None,\n",
        "    guidance_scale: float = None,\n",
        "    width: int = None,\n",
        "    height: int = None,\n",
        "    save_path: Optional[Path] = None\n",
        ") -> Tuple[Image.Image, Dict]:\n",
        "    \"\"\"\n",
        "    Generate a single image from a text prompt using SDXL.\n",
        "    \n",
        "    Args:\n",
        "        prompt: Positive text prompt describing the desired image\n",
        "        negative_prompt: Negative prompt to avoid unwanted features\n",
        "        seed: Random seed for reproducibility\n",
        "        num_inference_steps: Number of denoising steps (uses CONFIG default if None)\n",
        "        guidance_scale: Guidance scale (uses CONFIG default if None)\n",
        "        width: Output image width (uses CONFIG default if None)\n",
        "        height: Output image height (uses CONFIG default if None)\n",
        "        save_path: Optional path to save the image\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (PIL Image, metadata dictionary)\n",
        "    \"\"\"\n",
        "    # Set random seed for reproducibility\n",
        "    generator = torch.Generator(device=CONFIG['device']).manual_seed(seed)\n",
        "    \n",
        "    # Use config defaults if not specified\n",
        "    num_inference_steps = num_inference_steps or CONFIG['num_inference_steps']\n",
        "    guidance_scale = guidance_scale or CONFIG['guidance_scale']\n",
        "    width = width or CONFIG['width']\n",
        "    height = height or CONFIG['height']\n",
        "    \n",
        "    # Generate image with autocast for mixed precision\n",
        "    with autocast(CONFIG['device']):\n",
        "        result = pipe(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            width=width,\n",
        "            height=height,\n",
        "            generator=generator,\n",
        "        )\n",
        "    \n",
        "    image = result.images[0]\n",
        "    \n",
        "    # Create metadata\n",
        "    metadata = {\n",
        "        'prompt': prompt,\n",
        "        'negative_prompt': negative_prompt,\n",
        "        'seed': seed,\n",
        "        'num_inference_steps': num_inference_steps,\n",
        "        'guidance_scale': guidance_scale,\n",
        "        'width': width,\n",
        "        'height': height,\n",
        "        'model': CONFIG['model_id'],\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "    }\n",
        "    \n",
        "    # Save if path provided\n",
        "    if save_path:\n",
        "        image.save(save_path)\n",
        "        # Save metadata as JSON\n",
        "        metadata_path = save_path.with_suffix('.json')\n",
        "        with open(metadata_path, 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "    \n",
        "    return image, metadata\n",
        "\n",
        "print(\"Generation function defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch generation function defined successfully!\n"
          ]
        }
      ],
      "source": [
        "def generate_batch(\n",
        "    prompts: List[str],\n",
        "    negative_prompt: str = NEGATIVE_PROMPT,\n",
        "    base_seed: int = SEED,\n",
        "    use_varying_seeds: bool = True\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Generate images for a list of prompts.\n",
        "    \n",
        "    Args:\n",
        "        prompts: List of positive prompts\n",
        "        negative_prompt: Negative prompt to use for all generations\n",
        "        base_seed: Base seed value (each prompt may use base_seed + index if varying)\n",
        "        use_varying_seeds: If True, use different seed for each prompt\n",
        "    \n",
        "    Returns:\n",
        "        List of metadata dictionaries for successful generations\n",
        "    \"\"\"\n",
        "    generation_log = []\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    \n",
        "    print(f\"Generating {len(prompts)} images...\")\n",
        "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "    \n",
        "    for i, prompt in enumerate(tqdm(prompts, desc=\"Generating\")):\n",
        "        try:\n",
        "            # Determine seed for this generation\n",
        "            seed = base_seed + i if use_varying_seeds else base_seed\n",
        "            \n",
        "            # Create filename: timestamp_promptID_seed.png\n",
        "            prompt_id = str(i).zfill(3)  # Zero-padded 3-digit ID\n",
        "            filename = f\"{timestamp}_{prompt_id}_{seed}.png\"\n",
        "            save_path = OUTPUT_DIR / filename\n",
        "            \n",
        "            # Generate image\n",
        "            image, metadata = generate_image(\n",
        "                prompt=prompt,\n",
        "                negative_prompt=negative_prompt,\n",
        "                seed=seed,\n",
        "                save_path=save_path\n",
        "            )\n",
        "            \n",
        "            # Add file info to metadata\n",
        "            metadata['filename'] = filename\n",
        "            metadata['prompt_id'] = prompt_id\n",
        "            metadata['success'] = True\n",
        "            \n",
        "            generation_log.append(metadata)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"\\nError generating image for prompt {i}: {prompt}\")\n",
        "            print(f\"Error: {str(e)}\")\n",
        "            \n",
        "            # Log failure\n",
        "            generation_log.append({\n",
        "                'prompt_id': str(i).zfill(3),\n",
        "                'prompt': prompt,\n",
        "                'success': False,\n",
        "                'error': str(e),\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "            })\n",
        "            continue\n",
        "    \n",
        "    # Save generation log\n",
        "    log_path = LOGS_DIR / f\"generation_log_{timestamp}.json\"\n",
        "    with open(log_path, 'w') as f:\n",
        "        json.dump({\n",
        "            'timestamp': timestamp,\n",
        "            'total_prompts': len(prompts),\n",
        "            'successful': sum(1 for m in generation_log if m.get('success', False)),\n",
        "            'failed': sum(1 for m in generation_log if not m.get('success', True)),\n",
        "            'config': CONFIG,\n",
        "            'generations': generation_log,\n",
        "        }, f, indent=2)\n",
        "    \n",
        "    print(f\"\\nBatch generation complete!\")\n",
        "    print(f\"Successful: {sum(1 for m in generation_log if m.get('success', False))}\")\n",
        "    print(f\"Failed: {sum(1 for m in generation_log if not m.get('success', True))}\")\n",
        "    print(f\"Generation log saved to: {log_path}\")\n",
        "    \n",
        "    return generation_log\n",
        "\n",
        "print(\"Batch generation function defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Execute Batch Generation {#execute}\n",
        "\n",
        "Run the batch generation process.\n",
        "\n",
        "**Note:** This will generate images for all prompts in POSITIVE_PROMPTS.\n",
        "Expected runtime: ~[X] seconds per image on [GPU model].\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ready to generate. Uncomment the code above to execute batch generation.\n"
          ]
        }
      ],
      "source": [
        "# Execute batch generation\n",
        "# Uncomment the line below to run\n",
        "\n",
        "# generation_log = generate_batch(\n",
        "#     prompts=POSITIVE_PROMPTS,\n",
        "#     negative_prompt=NEGATIVE_PROMPT,\n",
        "#     base_seed=SEED,\n",
        "#     use_varying_seeds=True\n",
        "# )\n",
        "\n",
        "print(\"Ready to generate. Uncomment the code above to execute batch generation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. Test Single Generation {#test}\n",
        "\n",
        "Optional: Test generation with a single prompt before running full batch.\n",
        "\n",
        "Useful for:\n",
        "- Verifying pipeline is working correctly\n",
        "- Testing prompt variations\n",
        "- Quick iteration on prompt engineering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test single generation\n",
        "# Uncomment to test\n",
        "\n",
        "# test_prompt = POSITIVE_PROMPTS[0]\n",
        "# print(f\"Testing with prompt: {test_prompt}\")\n",
        "# \n",
        "# test_image, test_metadata = generate_image(\n",
        "#     prompt=test_prompt,\n",
        "#     negative_prompt=NEGATIVE_PROMPT,\n",
        "#     seed=42\n",
        "# )\n",
        "# \n",
        "# # Display image\n",
        "# display(test_image)\n",
        "# \n",
        "# print(f\"\\nMetadata:\")\n",
        "# print(json.dumps(test_metadata, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 9. Results Summary {#results}\n",
        "\n",
        "View summary statistics from generation log.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and display generation log summary\n",
        "# Uncomment to view results\n",
        "\n",
        "# import glob\n",
        "# log_files = sorted(glob.glob(str(LOGS_DIR / 'generation_log_*.json')))\n",
        "# \n",
        "# if log_files:\n",
        "#     latest_log = log_files[-1]\n",
        "#     with open(latest_log, 'r') as f:\n",
        "#         log_data = json.load(f)\n",
        "#     \n",
        "#     print(f\"Latest generation log: {latest_log}\")\n",
        "#     print(f\"Total prompts: {log_data['total_prompts']}\")\n",
        "#     print(f\"Successful: {log_data['successful']}\")\n",
        "#     print(f\"Failed: {log_data['failed']}\")\n",
        "#     print(f\"Success rate: {log_data['successful']/log_data['total_prompts']*100:.1f}%\")\n",
        "# else:\n",
        "#     print(\"No generation logs found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Notes and Troubleshooting\n",
        "\n",
        "### Memory Issues\n",
        "- If you encounter CUDA out of memory errors, try:\n",
        "  - Reducing batch_size in CONFIG\n",
        "  - Reducing image dimensions (width, height)\n",
        "  - Enabling `enable_model_cpu_offload()` instead of `enable_attention_slicing()`\n",
        "\n",
        "### Reproducibility\n",
        "- Same seed + same prompt + same parameters = same output\n",
        "- Use `use_varying_seeds=False` in `generate_batch()` to test reproducibility\n",
        "\n",
        "### Output Organization\n",
        "- Generated images: `results/baseline/`\n",
        "- Metadata JSON files: `results/baseline/` (alongside images)\n",
        "- Generation logs: `logs/generation_log_*.json`\n",
        "\n",
        "### Performance\n",
        "- Typical generation time: ~10-20 seconds per image on modern GPU\n",
        "- CPU generation is much slower (~5-10 minutes per image)\n",
        "- Consider batch size and generation count when estimating total time\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
